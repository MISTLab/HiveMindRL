{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "from bandit import BanditSigmoid, BanditLinear\n",
    "from population_simulation import imitaton_of_success, weighted_voter_rule\n",
    "from analytical_solutions import replicator_dynamic\n",
    "\n",
    "plt.rcParams[\"pdf.fonttype\"] = 42\n",
    "plt.rcParams[\"ps.fonttype\"] = 42\n",
    "\n",
    "plt.rcParams[\"text.latex.preamble\"] = r\"\\usepackage{lmodern} \\usepackage{amsmath}\"\n",
    "# Options\n",
    "params = {\n",
    "    \"text.usetex\": True,\n",
    "    \"font.size\": 11,\n",
    "    \"font.family\": \"lmodern\",\n",
    "    #   'text.latex.unicode': True,\n",
    "}\n",
    "plt.rcParams.update(params)\n",
    "\n",
    "colors = [\"#1f77b4\", \"#ff7f0e\", \"#854d05\", \"#00008b\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supp old population plots\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "population_sizes = [10, 1000]\n",
    "steps = 100\n",
    "seeds = 100\n",
    "\n",
    "\n",
    "for name in [\"evenly spaced\", \"near zero\", \"near one\"]:\n",
    "    for population_size in population_sizes:\n",
    "        bandit = Bandit(name=name)\n",
    "        mean_rewards_vr, optimal_action_vr = imitaton_of_success(\n",
    "            steps, population_size, seeds, bandit\n",
    "        )\n",
    "        mean_rewards_wvr, optimal_action_wvr = weighted_voter_rule(\n",
    "            steps, population_size, seeds, bandit\n",
    "        )\n",
    "        mean_rewards_trd, optimal_action_trd = replicator_dynamic(\n",
    "            1, bandit, steps, trd=True\n",
    "        )\n",
    "        mean_rewards_mrd, optimal_action_mrd = replicator_dynamic(\n",
    "            1, bandit, steps, trd=False\n",
    "        )\n",
    "\n",
    "        # plot stuff\n",
    "        plt.rcParams[\"figure.figsize\"] = [7, 2.5]\n",
    "        fig = plt.figure(constrained_layout=True)\n",
    "        gs = fig.add_gridspec(2, 3)\n",
    "        ax0 = fig.add_subplot(gs[:, 0])\n",
    "        ax1 = fig.add_subplot(gs[0, 1])\n",
    "        ax2 = fig.add_subplot(gs[1, 1])\n",
    "        ax3 = fig.add_subplot(gs[:, 2])\n",
    "        fontsize = 10\n",
    "        plt.rcParams.update({\"font.size\": fontsize})\n",
    "        ax2.sharex(ax1)\n",
    "        ax3.sharex(ax1)\n",
    "\n",
    "        # axs[2].sharex(axs[3])\n",
    "\n",
    "        colors = [\"#1f77b4\", \"#ff7f0e\", \"#ff4b33\", \"#00008b\"]\n",
    "        lines_labels = []\n",
    "\n",
    "        max_vr = max(mean_rewards_vr.mean(axis=0) + mean_rewards_vr.std(axis=0))\n",
    "        max_wvr = max(mean_rewards_wvr.mean(axis=0) + mean_rewards_wvr.std(axis=0))\n",
    "        max_reward = max_vr if max_vr > max_wvr else max_wvr\n",
    "        min_vr = min(mean_rewards_vr.mean(axis=0) - mean_rewards_vr.std(axis=0))\n",
    "        min_wvr = min(mean_rewards_wvr.mean(axis=0) - mean_rewards_wvr.std(axis=0))\n",
    "        min_reward = min_vr if min_vr < min_wvr else min_wvr\n",
    "\n",
    "        reward_steps = (max_reward - min_reward) / 6 + 0.1 / 6\n",
    "        reward_ticks = [min_reward - 0.05 + i * reward_steps for i in range(7)]\n",
    "\n",
    "        step_x = steps / 5\n",
    "\n",
    "        ax0.plot(\n",
    "            range(steps),\n",
    "            mean_rewards_trd,\n",
    "            label=\"TRD\",\n",
    "            color=colors[3],\n",
    "            linestyle=\"dotted\",\n",
    "            linewidth=2,\n",
    "            alpha=0.5,\n",
    "        )\n",
    "        ax0.plot(\n",
    "            range(steps),\n",
    "            mean_rewards_mrd,\n",
    "            label=\"MRD\",\n",
    "            color=colors[2],\n",
    "            linestyle=\"dotted\",\n",
    "            linewidth=2,\n",
    "            alpha=0.5,\n",
    "        )\n",
    "        ax0.set_ylim(reward_ticks[0], reward_ticks[-1])\n",
    "        ax0.set_yticks(reward_ticks)\n",
    "        ax0.set_yticklabels([f\"{i:.2f}\" for i in reward_ticks], fontsize=fontsize)\n",
    "        ax0.set_xticks([i for i in np.arange(0, steps + step_x, step_x)])\n",
    "        ax0.set_ylabel(\"Payoff/quality\", fontsize=fontsize)\n",
    "\n",
    "        ax0.set_xticklabels(\n",
    "            [f\"{i:.0f}\" for i in np.arange(0, steps + step_x, step_x)],\n",
    "            fontsize=fontsize,\n",
    "        )\n",
    "        ax0.set_xlabel(\"Time\", fontsize=fontsize)\n",
    "\n",
    "        # show only three points in the y axis\n",
    "        small_ticks = [reward_ticks[0], reward_ticks[int(7 / 2)], reward_ticks[-1]]\n",
    "        ax1.set_ylim(reward_ticks[0], reward_ticks[-1])\n",
    "        ax2.set_ylim(reward_ticks[0], reward_ticks[-1])\n",
    "        ax1.set_yticks(small_ticks)\n",
    "        ax1.set_yticklabels([f\"{i:.2f}\" for i in small_ticks], fontsize=8)\n",
    "        ax2.set_yticks(small_ticks)\n",
    "        ax2.set_yticklabels([f\"{i:.2f}\" for i in small_ticks], fontsize=8)\n",
    "\n",
    "        ax1.tick_params(labelbottom=False)\n",
    "        ax2.set_xticks([i for i in np.arange(0, steps + step_x, step_x)])\n",
    "        ax2.set_xticklabels(\n",
    "            [f\"{i:.0f}\" for i in np.arange(0, steps + step_x, step_x)],\n",
    "            fontsize=fontsize,\n",
    "        )\n",
    "        ax2.set_xlabel(\"Steps\", fontsize=fontsize)\n",
    "\n",
    "        ax1.plot(\n",
    "            range(steps),\n",
    "            mean_rewards_vr.mean(axis=0),\n",
    "            label=r\"$R_{\\text{success}}$\",\n",
    "            color=colors[0],\n",
    "            alpha=0.5,\n",
    "            linewidth=4,\n",
    "        )\n",
    "        ax1.plot(\n",
    "            range(steps),\n",
    "            mean_rewards_trd,\n",
    "            label=\"TRD\",\n",
    "            color=colors[3],\n",
    "            linestyle=\"dotted\",\n",
    "            linewidth=2,\n",
    "            alpha=0.5,\n",
    "        )\n",
    "        ax1.fill_between(\n",
    "            range(steps),\n",
    "            mean_rewards_vr.mean(axis=0) - mean_rewards_vr.std(axis=0),\n",
    "            mean_rewards_vr.mean(axis=0) + mean_rewards_vr.std(axis=0),\n",
    "            facecolor=colors[0],\n",
    "            alpha=0.2,\n",
    "        )\n",
    "        ax2.plot(\n",
    "            range(steps),\n",
    "            mean_rewards_wvr.mean(axis=0),\n",
    "            label=r\"$R_{\\text{wvoter}}$\",\n",
    "            color=colors[1],\n",
    "            alpha=0.5,\n",
    "            linewidth=4,\n",
    "        )\n",
    "        ax2.plot(\n",
    "            range(steps),\n",
    "            mean_rewards_mrd,\n",
    "            label=\"MRD\",\n",
    "            color=colors[2],\n",
    "            linestyle=\"dotted\",\n",
    "            linewidth=2,\n",
    "            alpha=0.5,\n",
    "        )\n",
    "        ax2.fill_between(\n",
    "            range(steps),\n",
    "            mean_rewards_wvr.mean(axis=0) - mean_rewards_wvr.std(axis=0),\n",
    "            mean_rewards_wvr.mean(axis=0) + mean_rewards_wvr.std(axis=0),\n",
    "            facecolor=colors[1],\n",
    "            alpha=0.2,\n",
    "        )\n",
    "\n",
    "        ax3.plot(\n",
    "            range(steps),\n",
    "            optimal_action_vr.mean(axis=0) * 100,\n",
    "            label=r\"$R_{\\text{success}}$\",\n",
    "            color=colors[0],\n",
    "            alpha=0.7,\n",
    "        )\n",
    "        ax3.plot(\n",
    "            range(steps),\n",
    "            optimal_action_trd * 100,\n",
    "            label=\"TRD\",\n",
    "            color=colors[3],\n",
    "            linestyle=\"dotted\",\n",
    "            linewidth=2,\n",
    "            alpha=0.5,\n",
    "        )\n",
    "        ax3.plot(\n",
    "            range(steps),\n",
    "            optimal_action_mrd * 100,\n",
    "            label=\"MRD\",\n",
    "            color=colors[2],\n",
    "            linestyle=\"dotted\",\n",
    "            linewidth=2,\n",
    "            alpha=0.5,\n",
    "        )\n",
    "\n",
    "        ax3.plot(\n",
    "            range(steps),\n",
    "            optimal_action_wvr.mean(axis=0) * 100,\n",
    "            label=r\"$R_{\\text{wvoter}}$\",\n",
    "            color=colors[1],\n",
    "            alpha=0.7,\n",
    "        )\n",
    "        ax3.yaxis.tick_right()\n",
    "        ax3.yaxis.set_label_position(\"right\")\n",
    "        ax3.set_yticks([i for i in np.arange(20, 120, 20)])\n",
    "        ax3.set_yticklabels([f\"{i}\" for i in np.arange(20, 120, 20)], fontsize=fontsize)\n",
    "        ax3.set_ylabel(r\"$\\%$ Optimal type\", fontsize=fontsize, rotation=270)\n",
    "        ax3.set_xlabel(\"Steps\", fontsize=fontsize)\n",
    "\n",
    "        if population_size == 10:\n",
    "            population_size_name = \"Small\"\n",
    "        else:\n",
    "            population_size_name = \"Large\"\n",
    "\n",
    "        if name == \"evenly spaced\":\n",
    "            title_name = \"Spread\"\n",
    "            title = (\n",
    "                r\"$q^{\\pi}_a$'s: \"\n",
    "                + str(title_name)\n",
    "                + r\", $\\mathcal{P}$: \"\n",
    "                + str(population_size_name)\n",
    "            )\n",
    "\n",
    "        elif name == \"near zero\":\n",
    "            title_name = \"Low\"\n",
    "            title = (\n",
    "                r\"$q^{\\pi}_a$'s: \"\n",
    "                + str(title_name)\n",
    "                + r\", $\\mathcal{P}$: \"\n",
    "                + str(population_size_name)\n",
    "            )\n",
    "\n",
    "        else:\n",
    "            title_name = \"High\"\n",
    "            title = (\n",
    "                r\"$q^{\\pi}_a$'s: \"\n",
    "                + str(title_name)\n",
    "                + r\", $\\mathcal{P}$: \"\n",
    "                + str(population_size_name)\n",
    "            )\n",
    "\n",
    "        # set common x-axis label\n",
    "        if (population_size_name == \"Large\") and (\n",
    "            title_name == \"High\" or title_name == \"Spread\"\n",
    "        ):\n",
    "            # fig.text(0.5, -0.02, 'Runs', ha='center', va='center', fontsize=fontsize)\n",
    "            fig.legend(\n",
    "                lines,\n",
    "                labels,\n",
    "                loc=\"upper center\",\n",
    "                bbox_to_anchor=(0.5, -0.04),\n",
    "                ncol=4,\n",
    "                fontsize=fontsize,\n",
    "            )\n",
    "\n",
    "        # set legend outside the plot\n",
    "        lines_labels = [ax0.get_legend_handles_labels()]\n",
    "        lines_labels.append(ax3.get_legend_handles_labels())\n",
    "        lines, labels = [sum(lol, []) for lol in zip(*lines_labels)]\n",
    "\n",
    "        fig.suptitle(title, fontsize=12)\n",
    "        plt.savefig(\n",
    "            f\"population_experiments_scenario_{title_name}_prallel_{population_size_name}.pdf\",\n",
    "            bbox_inches=\"tight\",\n",
    "            dpi=900,\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Refactor new population plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "population_sizes = [1000, 10]\n",
    "seeds = 1000\n",
    "fontsize = 10\n",
    "\n",
    "names = [\"near zero\", \"evenly spaced\", \"near one\"]\n",
    "names_dict = {\n",
    "    names[0]: \"Low\",\n",
    "    names[1]: \"Middle\",\n",
    "    names[2]: \"High\",\n",
    "}\n",
    "population_dict = {\n",
    "    population_sizes[0]: \"Small\",\n",
    "    population_sizes[1]: \"Large\",\n",
    "}\n",
    "steps_dict = {names[0]: 150, names[1]: 150, names[2]: 150}\n",
    "\n",
    "\n",
    "# plot stuff\n",
    "plt.rcParams.update({\"font.size\": fontsize})\n",
    "colors = [\"#1f77b4\", \"#00008b\", \"#ff7f0e\", \"#ff4b33\"]\n",
    "\n",
    "fig = plt.figure(figsize=(7, 2.5))  # or use tight_layout() later\n",
    "outer = gridspec.GridSpec(1, 3, width_ratios=[1, 1, 1], wspace=0.1)\n",
    "fig.text(\n",
    "    0.06,\n",
    "    0.5,\n",
    "    r\"$\\%$ Optimal action\",\n",
    "    horizontalalignment=\"center\",\n",
    "    verticalalignment=\"center\",\n",
    "    rotation=\"vertical\",\n",
    "    fontsize=fontsize,\n",
    ")\n",
    "fig.text(\n",
    "    0.92,\n",
    "    0.73,\n",
    "    r\"$\\mathcal{P}$: \" + f\"{population_sizes[0]}\",\n",
    "    horizontalalignment=\"center\",\n",
    "    verticalalignment=\"center\",\n",
    "    rotation=270,\n",
    "    fontsize=fontsize,\n",
    ")\n",
    "fig.text(\n",
    "    0.92,\n",
    "    0.28,\n",
    "    r\"$\\mathcal{P}$: \" + f\"{population_sizes[1]}\",\n",
    "    horizontalalignment=\"center\",\n",
    "    verticalalignment=\"center\",\n",
    "    rotation=270,\n",
    "    fontsize=fontsize,\n",
    ")\n",
    "fig.text(\n",
    "    0.5,\n",
    "    -0.02,\n",
    "    \"Steps\",\n",
    "    horizontalalignment=\"center\",\n",
    "    verticalalignment=\"center\",\n",
    "    fontsize=fontsize,\n",
    ")\n",
    "\n",
    "\n",
    "axs = []\n",
    "for i, name in enumerate(names):\n",
    "    inner = gridspec.GridSpecFromSubplotSpec(\n",
    "        2, 1, subplot_spec=outer[i], height_ratios=[1, 1], hspace=0.3\n",
    "    )\n",
    "    step = steps_dict[name]\n",
    "    steps_increment = int(step / 5)\n",
    "    if i == 0:\n",
    "        ax0 = fig.add_subplot(inner[0])\n",
    "        ax1 = fig.add_subplot(inner[1], sharey=ax0, sharex=ax0)\n",
    "\n",
    "        ax0.set_yticks(np.arange(20, 120, 20))\n",
    "        ax0.set_yticklabels([f\"{i:.0f}\" for i in np.arange(20, 120, 20)])\n",
    "        ax0.set_ylim(5, 110)\n",
    "\n",
    "        ax0.set_xticks(np.arange(0, step + steps_increment, steps_increment))\n",
    "        ax0.set_xticklabels(\n",
    "            [f\"{i:.0f}\" for i in np.arange(0, step + steps_increment, steps_increment)]\n",
    "        )\n",
    "        ax0.tick_params(labelbottom=False)\n",
    "\n",
    "    else:\n",
    "        ax0 = fig.add_subplot(inner[0], sharey=axs[0])\n",
    "        ax1 = fig.add_subplot(inner[1], sharex=ax0, sharey=axs[0])\n",
    "        ax0.set_xticks(np.arange(0, step + steps_increment, steps_increment))\n",
    "        ax0.set_xticklabels(\n",
    "            [f\"{i:.0f}\" for i in np.arange(0, step + steps_increment, steps_increment)]\n",
    "        )\n",
    "        ax0.tick_params(labelleft=False, labelbottom=False)\n",
    "        ax1.tick_params(labelleft=False)\n",
    "\n",
    "    ax0.set_title(r\"$q^{{\\pi}}_a$'s: \" + f\"{names_dict[names[i]]}\", fontsize=fontsize)\n",
    "\n",
    "    axs.append(ax0)\n",
    "    axs.append(ax1)\n",
    "\n",
    "for i, ax in enumerate(axs):\n",
    "\n",
    "    if i % 2 == 0:\n",
    "        j = 0\n",
    "        name = names[int(i / 2)]\n",
    "        bandit = BanditLinear(name=name)\n",
    "        population_size = population_sizes[0]\n",
    "    else:\n",
    "        j = 1\n",
    "        population_size = population_sizes[1]\n",
    "\n",
    "    step = steps_dict[name]\n",
    "\n",
    "    mean_rewards_trd, optimal_action_trd = replicator_dynamic(\n",
    "        delta=1, bandit=bandit, steps=step, trd=True\n",
    "    )\n",
    "\n",
    "    mean_rewards_mrd, optimal_action_mrd = replicator_dynamic(\n",
    "        delta=1, bandit=bandit, steps=step, trd=False\n",
    "    )\n",
    "\n",
    "    mean_rewards_is, optimal_action_is = imitaton_of_success(\n",
    "        steps=step, population_size=population_size, seeds=seeds, bandit=bandit\n",
    "    )\n",
    "\n",
    "    mean_rewards_wvr, optimal_action_wvr = weighted_voter_rule(\n",
    "        steps=step, population_size=population_size, seeds=seeds, bandit=bandit\n",
    "    )\n",
    "\n",
    "    ax.plot(\n",
    "        range(step),\n",
    "        optimal_action_trd * 100,\n",
    "        label=r\"TRD\",\n",
    "        color=colors[0],\n",
    "        linestyle=\"dotted\",\n",
    "        linewidth=2,\n",
    "        alpha=0.7,\n",
    "    )\n",
    "    ax.plot(\n",
    "        range(step),\n",
    "        optimal_action_mrd * 100,\n",
    "        label=r\"MRD\",\n",
    "        color=colors[2],\n",
    "        linestyle=\"dotted\",\n",
    "        linewidth=2,\n",
    "        alpha=0.7,\n",
    "    )\n",
    "\n",
    "    ax.plot(\n",
    "        range(step),\n",
    "        optimal_action_wvr.mean(axis=0) * 100,\n",
    "        label=r\"$R_{\\text{wvoter}}$\",\n",
    "        color=colors[2],\n",
    "        linewidth=1,\n",
    "        alpha=0.6,\n",
    "    )\n",
    "\n",
    "    ax.fill_between(\n",
    "        range(step),\n",
    "        optimal_action_wvr.mean(axis=0) * 100 - optimal_action_wvr.std(axis=0) * 100,\n",
    "        optimal_action_wvr.mean(axis=0) * 100 + optimal_action_wvr.std(axis=0) * 100,\n",
    "        facecolor=colors[2],\n",
    "        alpha=0.2,\n",
    "    )\n",
    "\n",
    "    ax.plot(\n",
    "        range(step),\n",
    "        optimal_action_is.mean(axis=0) * 100,\n",
    "        label=r\"$R_{\\text{success}}$\",\n",
    "        color=colors[0],\n",
    "        linewidth=1,\n",
    "        alpha=0.6,\n",
    "    )\n",
    "\n",
    "    ax.fill_between(\n",
    "        range(step),\n",
    "        optimal_action_is.mean(axis=0) * 100 - optimal_action_is.std(axis=0) * 100,\n",
    "        optimal_action_is.mean(axis=0) * 100 + optimal_action_is.std(axis=0) * 100,\n",
    "        facecolor=colors[0],\n",
    "        alpha=0.2,\n",
    "    )\n",
    "\n",
    "\n",
    "lines_labels = [axs[0].get_legend_handles_labels(), axs[1].get_legend_handles_labels()]\n",
    "lines, labels = [sum(lol, []) for lol in zip(*lines_labels)]\n",
    "unique = dict(zip(labels, lines))\n",
    "\n",
    "fig.legend(\n",
    "    unique.values(),\n",
    "    unique.keys(),\n",
    "    loc=\"upper center\",\n",
    "    bbox_to_anchor=(0.5, -0.04),\n",
    "    ncol=4,\n",
    "    fontsize=fontsize,\n",
    ")\n",
    "\n",
    "\n",
    "plt.savefig(f\"population_experiments.pdf\", bbox_inches=\"tight\", dpi=900)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Effect of changing the population sizes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "population_sizes = [10, 20, 50, 100, 500, 1000]\n",
    "# population_sizes = [10, 1000]\n",
    "steps = 250\n",
    "seeds = 1000\n",
    "name = \"evenly spaced\"\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(6, 3))\n",
    "colors = [\"#1f77b4\", \"#ff7f0e\", \"#854d05\", \"#00008b\", \"#2ca02c\", \"#e00e0e\", \"#e00ebd\"]\n",
    "i = 0\n",
    "bandit = BanditLinear(name=name)\n",
    "\n",
    "\n",
    "for population_size in population_sizes:\n",
    "    mean_rewards_wvr, optimal_action_wvr = weighted_voter_rule(\n",
    "        steps, population_size, seeds, bandit, neighbourhood_size=population_size\n",
    "    )\n",
    "    ax.plot(\n",
    "        range(steps),\n",
    "        optimal_action_wvr.mean(axis=0) * 100,\n",
    "        label=r\"$N$\" f\"={population_size}\",\n",
    "        color=colors[i],\n",
    "        alpha=0.7,\n",
    "        linewidth=3,\n",
    "    )\n",
    "\n",
    "    # ax.fill_between(\n",
    "    #     range(steps),\n",
    "    #     optimal_action_wvr.mean(axis=0) * 100 - optimal_action_wvr.std(axis=0) * 100,\n",
    "    #     optimal_action_wvr.mean(axis=0) * 100 + optimal_action_wvr.std(axis=0) * 100,\n",
    "    #     color=colors[i],\n",
    "    #     alpha=0.2,\n",
    "    # )\n",
    "\n",
    "    i += 1\n",
    "\n",
    "# simulate MRD\n",
    "mean_rewards_mrd, optimal_population_mrd = replicator_dynamic(\n",
    "    1, bandit, steps, trd=False\n",
    ")\n",
    "ax.plot(\n",
    "    range(steps),\n",
    "    optimal_population_mrd * 100,\n",
    "    label=\"MRD\",\n",
    "    color=colors[-1],\n",
    "    linestyle=\"dotted\",\n",
    "    linewidth=2,\n",
    "    alpha=0.5,\n",
    ")\n",
    "\n",
    "ax.set_ylabel(r\"Average $\\%$ optimal type\", fontsize=10)\n",
    "ax.set_xlabel(\"Steps\", fontsize=10)\n",
    "ax.set_yticks([i for i in np.arange(20, 120, 20)])\n",
    "ax.set_yticklabels([f\"{i}\" for i in np.arange(20, 120, 20)], fontsize=10)\n",
    "ax.legend(loc=\"lower right\", fontsize=10, ncols=3)\n",
    "\n",
    "plt.savefig(f\"R_wvoter_scenario_evenly_spaced.pdf\", bbox_inches=\"tight\", dpi=900)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bandit = BanditLinear(name=\"near one\")\n",
    "step = 100\n",
    "seeds = 10\n",
    "population_size = 2000000\n",
    "\n",
    "\n",
    "mean_rewards_trd, optimal_action_trd = replicator_dynamic(\n",
    "    delta=1, bandit=bandit, steps=step, trd=True\n",
    ")\n",
    "\n",
    "mean_rewards_mrd, optimal_action_mrd = replicator_dynamic(\n",
    "    delta=1, bandit=bandit, steps=step, trd=False\n",
    ")\n",
    "\n",
    "mean_rewards_is, optimal_action_is = imitaton_of_success(\n",
    "    steps=step, population_size=population_size, seeds=seeds, bandit=bandit\n",
    ")\n",
    "\n",
    "mean_rewards_wvr, optimal_action_wvr = weighted_voter_rule(\n",
    "    steps=step, population_size=population_size, seeds=seeds, bandit=bandit\n",
    ")\n",
    "\n",
    "fig = plt.figure(figsize=(10, 5), dpi=100)\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "\n",
    "ax.plot(\n",
    "    range(step),\n",
    "    optimal_action_trd * 100,\n",
    "    label=\"TRD\",\n",
    "    color=colors[0],\n",
    "    linestyle=\"dotted\",\n",
    "    linewidth=2,\n",
    "    alpha=0.7,\n",
    ")\n",
    "\n",
    "ax.plot(\n",
    "    range(step),\n",
    "    optimal_action_mrd * 100,\n",
    "    label=\"MRD\",\n",
    "    color=colors[2],\n",
    "    linestyle=\"dotted\",\n",
    "    linewidth=2,\n",
    "    alpha=0.7,\n",
    ")\n",
    "\n",
    "ax.plot(\n",
    "    range(step),\n",
    "    optimal_action_is.mean(axis=0) * 100,\n",
    "    label=r\"$R_{\\text{success}}$ for $\\mathcal{P}$: \" + f\"{population_size}\",\n",
    "    linewidth=1,\n",
    "    alpha=0.6,\n",
    ")\n",
    "\n",
    "ax.fill_between(\n",
    "    range(step),\n",
    "    optimal_action_is.mean(axis=0) * 100 - optimal_action_is.std(axis=0) * 100,\n",
    "    optimal_action_is.mean(axis=0) * 100 + optimal_action_is.std(axis=0) * 100,\n",
    "    alpha=0.2,\n",
    ")\n",
    "\n",
    "ax.plot(\n",
    "    range(step),\n",
    "    optimal_action_wvr.mean(axis=0) * 100,\n",
    "    label=r\"$R_{\\text{wvoter}}$ for $\\mathcal{P}$: \" + f\"{population_size}\",\n",
    "    linewidth=1,\n",
    "    alpha=0.6,\n",
    ")\n",
    "\n",
    "ax.fill_between(\n",
    "    range(step),\n",
    "    optimal_action_wvr.mean(axis=0) * 100 - optimal_action_wvr.std(axis=0) * 100,\n",
    "    optimal_action_wvr.mean(axis=0) * 100 + optimal_action_wvr.std(axis=0) * 100,\n",
    "    alpha=0.2,\n",
    ")\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bandit = BanditLinear(name=\"near one\")\n",
    "\n",
    "step = 300\n",
    "seeds = 100\n",
    "\n",
    "\n",
    "mean_rewards_wvr, optimal_action_wvr_10 = weighted_voter_rule(\n",
    "    steps=step, population_size=1000, seeds=seeds, bandit=bandit, neighbourhood_size=10\n",
    ")\n",
    "\n",
    "mean_rewards_wvr, optimal_action_wvr_1 = weighted_voter_rule(\n",
    "    steps=step, population_size=1000, seeds=seeds, bandit=bandit, neighbourhood_size=1\n",
    ")\n",
    "\n",
    "\n",
    "mean_rewards_wvr, optimal_action_wvr_1000 = weighted_voter_rule(\n",
    "    steps=step,\n",
    "    population_size=1000,\n",
    "    seeds=seeds,\n",
    "    bandit=bandit,\n",
    "    neighbourhood_size=1000,\n",
    ")\n",
    "\n",
    "mean_rewards_mrd, optimal_action_mrd = replicator_dynamic(\n",
    "    delta=1, bandit=bandit, steps=step, trd=False\n",
    ")\n",
    "\n",
    "\n",
    "fig = plt.figure(figsize=(10, 5), dpi=100)\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "\n",
    "\n",
    "ax.plot(\n",
    "    range(step),\n",
    "    optimal_action_mrd * 100,\n",
    "    label=\"TRD\",\n",
    "    linestyle=\"dotted\",\n",
    "    linewidth=2,\n",
    "    alpha=0.7,\n",
    ")\n",
    "\n",
    "ax.plot(\n",
    "    range(step),\n",
    "    optimal_action_wvr_10.mean(axis=0) * 100,\n",
    "    label=\"wvr_10\",\n",
    "    linestyle=\"dotted\",\n",
    "    linewidth=2,\n",
    "    alpha=0.7,\n",
    ")\n",
    "\n",
    "ax.plot(\n",
    "    range(step),\n",
    "    optimal_action_wvr_1000.mean(axis=0) * 100,\n",
    "    label=\"wvr_100\",\n",
    "    linestyle=\"dotted\",\n",
    "    linewidth=2,\n",
    "    alpha=0.7,\n",
    ")\n",
    "\n",
    "ax.plot(\n",
    "    range(step),\n",
    "    optimal_action_wvr_1.mean(axis=0) * 100,\n",
    "    label=\"wvr_2\",\n",
    "    linestyle=\"dotted\",\n",
    "    linewidth=2,\n",
    "    alpha=0.7,\n",
    ")\n",
    "\n",
    "ax.fill_between(\n",
    "    range(step),\n",
    "    optimal_action_wvr_1.mean(axis=0) * 100 - optimal_action_wvr_1.std(axis=0) * 100,\n",
    "    optimal_action_wvr_1.mean(axis=0) * 100 + optimal_action_wvr_1.std(axis=0) * 100,\n",
    "    alpha=0.2,\n",
    ")\n",
    "\n",
    "ax.fill_between(\n",
    "    range(step),\n",
    "    optimal_action_wvr_10.mean(axis=0) * 100 - optimal_action_wvr_10.std(axis=0) * 100,\n",
    "    optimal_action_wvr_10.mean(axis=0) * 100 + optimal_action_wvr_10.std(axis=0) * 100,\n",
    "    alpha=0.2,\n",
    ")\n",
    "\n",
    "ax.fill_between(\n",
    "    range(step),\n",
    "    optimal_action_wvr_1000.mean(axis=0) * 100\n",
    "    - optimal_action_wvr_1000.std(axis=0) * 100,\n",
    "    optimal_action_wvr_1000.mean(axis=0) * 100\n",
    "    + optimal_action_wvr_1000.std(axis=0) * 100,\n",
    "    alpha=0.2,\n",
    ")\n",
    "\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "bandit = BanditLinear(name=\"near one\", device=\"cpu\")\n",
    "\n",
    "step = 100\n",
    "seeds = 100\n",
    "population_size = 500\n",
    "\n",
    "\n",
    "mean_rewards_wvr, optimal_action_wvr_10 = weighted_voter_rule(\n",
    "    steps=step,\n",
    "    population_size=population_size,\n",
    "    seeds=seeds,\n",
    "    bandit=bandit,\n",
    "    neighbourhood_size=2,\n",
    "    disjoint_neighbourhood=False,\n",
    "    device=\"cpu\",\n",
    ")\n",
    "\n",
    "# mean_rewards_mrd, optimal_action_mrd = replicator_dynamic(\n",
    "#     delta=1, bandit=bandit, steps=step, trd=False\n",
    "# )\n",
    "\n",
    "# fig = plt.figure(figsize=(10, 5), dpi=100)\n",
    "# ax = fig.add_subplot(1, 1, 1)\n",
    "\n",
    "# ax.plot(\n",
    "#     range(step),\n",
    "#     optimal_action_wvr_10.mean(axis=0) * 100,\n",
    "#     label=\"wvr_10\",\n",
    "#     linestyle=\"dotted\",\n",
    "#     linewidth=2,\n",
    "#     alpha=0.7,\n",
    "# )\n",
    "\n",
    "# ax.plot(\n",
    "#     range(step),\n",
    "#     optimal_action_mrd * 100,\n",
    "#     label=\"MRD\",\n",
    "#     linestyle=\"dotted\",\n",
    "#     linewidth=2,\n",
    "#     alpha=0.7,\n",
    "# )\n",
    "\n",
    "# ax.fill_between(\n",
    "#     range(step),\n",
    "#     optimal_action_wvr_10.mean(axis=0) * 100 - optimal_action_wvr_10.std(axis=0) * 100,\n",
    "#     optimal_action_wvr_10.mean(axis=0) * 100 + optimal_action_wvr_10.std(axis=0) * 100,\n",
    "#     alpha=0.2,\n",
    "# )\n",
    "\n",
    "# plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from time import perf_counter\n",
    "\n",
    "weights = torch.ones((500, 499))\n",
    "\n",
    "quality_matrix = torch.zeros((500, 10), dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken: 0.0110 seconds\n"
     ]
    }
   ],
   "source": [
    "for _ in range(100):\n",
    "    _ = torch.multinomial(weights, num_samples=2, replacement=False)\n",
    "    _ = torch.distributions.Categorical(weights).sample()\n",
    "\n",
    "start = perf_counter()\n",
    "idx = torch.multinomial(weights, num_samples=2, replacement=False)\n",
    "pop_opinions = torch.distributions.Categorical(weights).sample()\n",
    "end = perf_counter()\n",
    "print(f\"Time taken: {end - start:.4f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9.9"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "0.0099 * 100 * 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken: 0.0223 seconds\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "end = time.time()\n",
    "print(f\"Time taken: {end - start:.4f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
